# Config file for q vs g jet classification

#------------------------------------------------------------------
# These parameters are used in the processing script
#------------------------------------------------------------------

# Some parameters of the q-g dataset
R: 0.4
pt: [500., 550.]
y_max: 1.701

# Size of q-g labeled data to load
n_total: 2100

# Nsubjettiness basis
K_max: 8

# Subjet basis...
N_max: 10
r: [0.01, 0.1] 

# Clustering Algorithm, The options are: 'kt_algorithm', 'antikt_algorithm' ,'cambridge_algorithm' 
Clustering_Alg: 'antikt_algorithm' 

# Laman Construction, The options are: 'naive' , '1N', '1N2N'
Laman_construction : 'naive' 

#------------------------------------------------------------------
# These parameters are used only in ML analysis
#------------------------------------------------------------------

# Size of q-g labeled data to load
n_train: 1500
n_val: 300
n_test: 300

# Classification labels
q_label: 'quark'                                    # label 1
g_label: 'gluon'                                    # label 0

# Define Nsubjettiness observable basis sets to train models
# The K-body phase space is (3K-4)-dimensional
K: [8]

# Select model: pfn, efn, subjet_linear, subjet_dnn, nsub_linear, nsub_dnn
models: [pfn, sub_pfn, efn, sub_dnn, laman_dnn, nsub_dnn, nsub_linear]

pfn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    epochs: 100
    batch_size: 500
    use_pids: True                                  # Use PID information (this option is currently ignored)

sub_pfn:

    # Network architecture parameters
    Phi_sizes: [100, 100 , 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    epochs: 100
    batch_size: 500
    use_pids: False                                 
    
efn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    learning_rate: 0.001
    epochs: 100
    batch_size: 500

sub_dnn:

    # Model hyperparameters
    learning_rate: [0.001]                          # (0.001 cf 1810.05165)
    loss: 'binary_crossentropy'                     # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']                           # measure accuracy during training
    batch_size: 500                   
    epochs: 100                                      # number of training epochs

laman_dnn:

    # Model hyperparameters
    learning_rate: [0.001]                           # (0.001 cf 1810.05165)
    loss: 'binary_crossentropy'                     # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']                           # measure accuracy during training
    batch_size: 500                   
    epochs: 100                                      # number of training epochs
    
   
nsub_dnn:

    # Model hyperparameters
    learning_rate: [1., 0.1, 0.01, 1.e-3, 1.e-4]    # (0.001 cf 1810.05165)
    loss: 'binary_crossentropy'                     # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']                           # measure accuracy during training
    batch_size: 500                    
    epochs: 30                                      # number of training epochs
    
nsub_linear:

    # Model hyperparameters -- SGDClassifier
    sgd_loss: 'hinge'                               # cost function
    sgd_penalty: ['l2', 'l1']                       # regularization term
    sgd_alpha: [1e-5, 1e-4, 1e-3]                   # regularization strength
    sgd_max_iter: 1000                              # max number of epochs
    sgd_tol: [1e-5, 1e-4, 1e-3]                     # criteria to stop training
    sgd_learning_rate: 'optimal'                    # learning schedule (learning rate decreases over time in proportion to alpha)
    sgd_early_stopping: False                       # whether to stop training based on validation score

    lda_tol: [1e-6, 1e-5, 1e-4, 1e-3, 5e-3, 1e-2]   # criteria to stop training

    # Hyperparameter tuning
    n_iter: 10                                      # number of random hyperparameter sets to try
    cv: 5                                           # number of cross-validation folds

