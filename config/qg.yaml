# Config file for q vs g jet classification

#------------------------------------------------------------------
# These parameters are used in the processing script
#------------------------------------------------------------------

dataset_type: 'qg'

# Some parameters of the q-g dataset

R: 0.4
pt: [1000., 1050.]  # The choices are [300, 350], [500, 550], [1000, 1050]
                    # For [500., 550.] we can load either Jesse's or Mateusz's. Check the processing script to see which one. (Maybe add a switch ?)
y_max: 1.701

# Size of q-g labeled data to load
n_total: 10000

# Load laman
laman: 'True'

# Nsubjettiness basis
K_max: 4

# Subjet basis: 'exclusive' or 'inclusive'
subjet_basis: 'inclusive'

# 1. Exclusive, fixed number of jets; It must be in the form of a list . Note: using the anti-kt alg. here gives a warning from fastjet
njet: [30]

# 2. Inclusive. Fixed value for the subjet radius; It must be in the form of a list. Choose radii r and cutoff N_max
N_max: [30]
r: [0.001]

# Clustering Algorithm, The options are: 'kt_algorithm', 'antikt_algorithm' ,'cambridge_algorithm' 
Clustering_Alg: 'antikt_algorithm' 

# Laman Construction, The options are: 'naive' , '1N', '1N2N'
Laman_construction : 'naive' 

# Run fully connected graph
fully_con_subjets : 'True'
fully_con_hadrons : 'True'

# Load Herwig Dataset for testing. Boolean variable
Herwig_dataset : 'False'

#------------------------------------------------------------------
# These parameters are used only in ML analysis
#------------------------------------------------------------------

# Size of q-g labeled data to load
n_train: 3000
n_val:   1000
n_test:  1000

# Classification labels
q_label: 'quark'                                    # label 1
g_label: 'gluon'                                    # label 0

# Define Nsubjettiness observable basis sets to train models
# The K-body phase space is (3K-4)-dimensional
K: [2,4]

# Select model: pfn, sub_pfn, efn, sub_efn, particle_gnn, sub_dnn, laman_dnn, nsub_dnn, nsub_linear
models: [particle_gnn]

pfn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    epochs: 50
    batch_size: 500
    use_pids: True                                  # Use PID information (this option is currently ignored)

sub_pfn:

    # Network architecture parameters
    Phi_sizes: [100, 100 , 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    epochs: 50
    batch_size: 500
    use_pids: False                                 
    
efn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    learning_rate: 0.001
    epochs: 50
    batch_size: 500

sub_efn:

    # Network architecture parameters
    Phi_sizes: [100, 100 , 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    learning_rate: 0.001
    epochs: 50
    batch_size: 500

particle_gnn:
    epochs: 20
    batch_size: 128

sub_dnn:

    # Model hyperparameters
    learning_rate: [0.001]                          # (0.001 cf 1810.05165)
    loss: 'binary_crossentropy'                     # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']                           # measure accuracy during training
    batch_size: 500                   
    epochs: 100                                      # number of training epochs

laman_dnn:

    # Model hyperparameters
    learning_rate: [0.001]                           # (0.001 cf 1810.05165)
    loss: 'binary_crossentropy'                     # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']                           # measure accuracy during training
    batch_size: 500                   
    epochs: 100                                      # number of training epochs
    
   
nsub_dnn:

    # Model hyperparameters
    learning_rate: [1., 0.1, 0.01, 1.e-3, 1.e-4]    # (0.001 cf 1810.05165)
    loss: 'binary_crossentropy'                     # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']                           # measure accuracy during training
    batch_size: 500                    
    epochs: 30                                      # number of training epochs
    
nsub_linear:

    # Model hyperparameters -- SGDClassifier
    sgd_loss: 'hinge'                               # cost function
    sgd_penalty: ['l2', 'l1']                       # regularization term
    sgd_alpha: [1e-5, 1e-4, 1e-3]                   # regularization strength
    sgd_max_iter: 1000                              # max number of epochs
    sgd_tol: [1e-5, 1e-4, 1e-3]                     # criteria to stop training
    sgd_learning_rate: 'optimal'                    # learning schedule (learning rate decreases over time in proportion to alpha)
    sgd_early_stopping: False                       # whether to stop training based on validation score

    lda_tol: [1e-6, 1e-5, 1e-4, 1e-3, 5e-3, 1e-2]   # criteria to stop training

    # Hyperparameter tuning
    n_iter: 10                                      # number of random hyperparameter sets to try
    cv: 5                                           # number of cross-validation folds
