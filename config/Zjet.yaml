# Config file for Z jet vs. QCD jet classification

#------------------------------------------------------------------
# These parameters are used in the processing script
#------------------------------------------------------------------

# File on hiccup at: /rstorage/ml/zjets_test/pythia_gen_qorgorZjet_mDT0.04_Zjet_0.root
#  - tree_Particle_gen: anti-kT jet with R=1.0 matched with Z
#  - tree_Particle_gen_mDT0.04: same anti-kT jet but cleaned up with Mass Drop Tagger (z_cut = 0.04) - a la std practice to do that I’ve seen (diffs only should affect low-pT’s)

# Some parameters of the Z-jet dataset

dataset_type: 'Zjet'

# Some parameters of the Z-QCD dataset

R: 0.8
pt: [500.,550.] # The choices are [300, 350], [500, 550], [1000, 1050], [1000, 1100]
y_max: 1.71
mj_min: 0
mj_max: 225

# Size of q-g labeled data to load, it has to be a multiple of 1000
n_total: 20000

# Load laman
laman: 'True'

# Nsubjettiness basis
K_max: 4

# Subjet basis: 'exclusive' or 'inclusive'
subjet_basis: 'inclusive'

# 1. Exclusive, fixed number of jets; It must be in the form of a list. Note: using the anti-kt alg. here gives a warning from fastjet
njet: [100, 50, 12, 8]

# 2. Inclusive. Fixed value for the subjet radius; It must be in the form of a list. Choose radii r and cutoff N_max
N_max: [20]
r: [0.001]

# Clustering Algorithm, The options are: 'kt_algorithm', 'antikt_algorithm' ,'cambridge_algorithm' 
Clustering_Alg: 'antikt_algorithm' 

# Laman Construction, The options are: 'naive' , '1N', '1N2N'
Laman_construction : 'naive' 

# Run fully connected graph
fully_con_subjets : False
fully_con_hadrons : True

# Load Herwig Dataset for testing. Boolean variable
Herwig_dataset : False

#------------------------------------------------------------------
# These parameters are used only in ML analysis
#------------------------------------------------------------------

# Size of q-g labeled data to load
n_train: 15000
n_val:   2500
n_test:  2500

# Classification labels
q_label: 'Z'                                      # label 1
g_label: 'QCD'                                    # label 0

# Define Nsubjettiness observable basis sets to train models
# The K-body phase space is (3K-4)-dimensional
K: [2,4]

# Select model: pfn, sub_pfn, efn, particle_gnn, sub_dnn, laman_dnn, nsub_dnn, nsub_linear, particle_net, pfn_pytorch 
models: [particle_net]

pfn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    epochs: 50
    batch_size: 500
    use_pids: True                                  # Use PID information (this option is currently ignored)

particle_net:
    epochs: 10
    batch_size: 128

sub_pfn:

    # Network architecture parameters
    Phi_sizes: [100, 100 , 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    epochs: 30
    batch_size: 500
    use_pids: False                                 
    
efn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    learning_rate: 0.001
    epochs: 100
    batch_size: 500

particle_gnn:
    epochs: 10
    batch_size: 128

sub_dnn:

    # Model hyperparameters
    learning_rate: [0.001]                          # (0.001 cf 1810.05165)
    loss: 'binary_crossentropy'                     # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']                           # measure accuracy during training
    batch_size: 500                   
    epochs: 100                                      # number of training epochs

laman_dnn:

    # Model hyperparameters
    learning_rate: [0.001]                           # (0.001 cf 1810.05165)
    loss: 'binary_crossentropy'                     # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']                           # measure accuracy during training
    batch_size: 500                   
    epochs: 100                                      # number of training epochs
    
   
nsub_dnn:

    # Model hyperparameters
    learning_rate: [1., 0.1, 0.01, 1.e-3, 1.e-4]    # (0.001 cf 1810.05165)
    loss: 'binary_crossentropy'                     # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']                           # measure accuracy during training
    batch_size: 500                    
    epochs: 30                                      # number of training epochs
    
nsub_linear:

    # Model hyperparameters -- SGDClassifier
    sgd_loss: 'hinge'                               # cost function
    sgd_penalty: ['l2', 'l1']                       # regularization term
    sgd_alpha: [1e-5, 1e-4, 1e-3]                   # regularization strength
    sgd_max_iter: 1000                              # max number of epochs
    sgd_tol: [1e-5, 1e-4, 1e-3]                     # criteria to stop training
    sgd_learning_rate: 'optimal'                    # learning schedule (learning rate decreases over time in proportion to alpha)
    sgd_early_stopping: False                       # whether to stop training based on validation score

    lda_tol: [1e-6, 1e-5, 1e-4, 1e-3, 5e-3, 1e-2]   # criteria to stop training

    # Hyperparameter tuning
    n_iter: 10                                      # number of random hyperparameter sets to try
    cv: 5                                           # number of cross-validation folds
